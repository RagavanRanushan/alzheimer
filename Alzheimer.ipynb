{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a056aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pathlib\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f30974",
   "metadata": {},
   "source": [
    "## Plan \n",
    "\n",
    "1) division des données\n",
    "\n",
    "2) generation d'images via gan\n",
    "\n",
    "3) 1er modele et resultat\n",
    "\n",
    "4) amélioration de la qualité des images via vae\n",
    "\n",
    "5) 1er modèle entrainer sur les images améliorer\n",
    "\n",
    "6) Prédiction avec YOLO\n",
    "\n",
    "## Division des données\n",
    "\n",
    "Nous disposant de 4 dossiers contenant chacune une catégorie d'image.\n",
    "* MildDemented contient 896 fichier\n",
    "* ModerateDemented:64 fichier\n",
    "* NonDemented: 3200 fichier\n",
    "* VeryMildDemented: 2240 fichier\n",
    "\n",
    "On remarque que les classes sont désequilibré notamment pour la catégorie ModerateDemented qui n'a que 64 images. \n",
    "\n",
    "\n",
    "Commençons par scindé ces dossiers en 3 dossiers afin d'avoir un jeu de donnée pour les étapes de validation, d'entrainement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93792f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On renomme les images afin de facilité la réorganisation\n",
    "path=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset\"\n",
    "folder_path=[]\n",
    "cat=[]\n",
    "for dirname,_, filename in os.walk(path):\n",
    "    folder_path.append(dirname)\n",
    "    cat.append(re.sub(\"\\\\\\\\\",\"/\",dirname).split('/')[-1])\n",
    "\n",
    "\n",
    "del cat[0]    \n",
    "del folder_path[0] #On retire le 1er élément qui correspond au chemin path\n",
    "\n",
    "for i in range(len(folder_path)):\n",
    "    folder_path[i]=re.sub(\"\\\\\\\\\",\"/\",folder_path[i]) #On corrige les antislash en slash\n",
    "\n",
    "    \n",
    "#On renomme les fichiers\n",
    "k=0 #Accumulateur pour parcourir la liste des labels\n",
    "for pat in folder_path:\n",
    "    j=0 #initalisationde l'accumulateur\n",
    "    for dirname ,_, filenames  in os.walk(pat):\n",
    "        for file in filenames:\n",
    "            a=re.sub(\"\\\\\\\\\",\"/\",os.path.join(pat,file))\n",
    "            b=re.sub(\"\\\\\\\\\",\"/\",os.path.join(pat,cat[k]+str(j)+'.jpg'))\n",
    "            #os.rename(a,b)\n",
    "            j+=1\n",
    "    k+=1\n",
    "\n",
    "#Créations de liste contenant les différents composante des futurs chemin d'accés\n",
    "l=[]\n",
    "cat=[]\n",
    "for dirname ,_, filenames  in os.walk(path):\n",
    "    cat.append(dirname.split(\"\\\\\")[-1])\n",
    "    l.append(dirname)\n",
    "del cat[0]\n",
    "a=[\"train_dir\",\"validation_dir\",\"test_dir\"]\n",
    "\n",
    "\n",
    "#Renommons toutes les fichier de chaque dossier afin de faciliter le copie.\n",
    "\n",
    "\n",
    "#Création des dossier vide et copie des images\n",
    "path2=re.sub(\"\\\\\\\\\",\"/\",os.path.join(path,\"SplitData\"))\n",
    "os.mkdir(path2)\n",
    "\n",
    "for A in a:\n",
    "    os.mkdir(re.sub(\"\\\\\\\\\",'/',os.path.join(path2,A)))\n",
    "\n",
    "for i in cat:\n",
    "    p0=re.sub(\"\\\\\\\\\",\"/\",os.path.join(path,i))\n",
    "    n=len(os.listdir(p0))#compte le nombre d'element du fichier i\n",
    "    fnames=os.listdir(p0)\n",
    "    for j in a:\n",
    "        p=re.sub(\"\\\\\\\\\",\"/\",os.path.join(path2,j,i))\n",
    "        os.mkdir(str(p))\n",
    "            #fnames=['{}.jpg' for k in range(0,2*n//4)]# 50% des données serviront à l'entrainement.\n",
    "        if j==\"train_dir\":\n",
    "            for k in range(0,2*n//4):\n",
    "            #for fname in fnames:\n",
    "                src=re.sub(\"\\\\\\\\\",\"/\",os.path.join(p0,fnames[k]))\n",
    "                dst=re.sub(\"\\\\\\\\\",\"/\",os.path.join(p,i+str(k)+\".jpg\"))\n",
    "                os.renames(src,dst)\n",
    "        elif j=='validation_dir':\n",
    "            for k in range(2*n//4,3*n//4):\n",
    "           # fnames=['{}.jpg' for k in range(2*n//4,3*n//4)]# 25% des données serviront à la validation.\n",
    "            #for fname in fnames:\n",
    "                src=re.sub(\"\\\\\\\\\",\"/\",os.path.join(p0,fnames[k]))\n",
    "                dst=re.sub(\"\\\\\\\\\",\"/\",os.path.join(p,i+str(k)+\".jpg\"))\n",
    "                os.renames(src,dst)\n",
    "        elif j=='test_dir':\n",
    "        #fnames=['{}.jpg' for k in range(3*n//4,n)]# 25% des données serviront à tester le modèle.\n",
    "       # for fname in fnames:\n",
    "            for k in range(3*n//4,n):\n",
    "                src=re.sub(\"\\\\\\\\\",\"/\",os.path.join(p0,fnames[k]))\n",
    "                dst=re.sub(\"\\\\\\\\\",\"/\",os.path.join(p,i+str(k)+\".jpg\"))\n",
    "                os.renames(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127e310",
   "metadata": {},
   "source": [
    "## Generation d'image via un réseau GAN\n",
    "\n",
    "Afin de résoudre le problème des classes déséquilibrés souligné plus tôt, on va créer des images à l'aide d'un réseau GAN.\n",
    "De plus pour éviter de créer des images appartenant aux autres catégorie nous les utiliserons pour entrainer le discriminant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset\"\n",
    "liste=[]\n",
    "for dirname,_,filenames in os.walk(path):\n",
    "    #if dirname=='ModerateDemented':\n",
    "    for file in filenames:\n",
    "        liste+=[re.sub(\"\\\\\\\\\",\"/\",os.path.join(dirname,file))]\n",
    "im=cv2.imread(liste[0])\n",
    "plt.imshow(im)\n",
    "im.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateur_model():\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(layers.Dense(208*176*3, use_bias=False, input_shape=[208,176,3])) #initialisation d'un vecteur bruit\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((208,176,3)))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(512,(5,5),strides=(2,2),padding='same',use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(256,(5,5),strides=(2,2),padding='same',use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128),(5,5),strides=(2,2),padding=\"same\",use_bias=False)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64),(5,5),strides=(2,2),padding=\"same\",use_bias=False)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1,(5,5),strides=(2,2),padding='same',use_bias=False,activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def discriminant_model():\n",
    "    model=tf.keras.Sequential()\n",
    "  \n",
    "    model.add(layers.Dense(4*4*1024, use_bias=False, input_shape=[208,176,3]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLu())\n",
    "    \n",
    "   \n",
    "    model.add(layers.Conv2D(256,(5,5),stides(2,2),padding='same')\n",
    "    model.add(layers.LeakyRelu()) \n",
    "    \n",
    "    model.add(layers.Conv2D(128,(5,5),stides(2,2),padding='same')\n",
    "    model.add(layers.LeakyRelu()) \n",
    "    \n",
    "    model.add(layers.Conv2D(64,(5,5),stides(2,2),padding='same')\n",
    "    model.add(layers.LeakyRelu()) \n",
    "    model.add(layers.Dense(1,activation'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47761fca",
   "metadata": {},
   "source": [
    "## Pré traitement des données\n",
    "\n",
    "Convertissons les images en tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "115ca5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3173 images belonging to 4 classes.\n",
      "Found 1587 images belonging to 4 classes.\n",
      "Found 1587 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset/SplitData/train_dir\"\n",
    "valid_dir=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset/SplitData/validation_dir\"\n",
    "test_dir=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset/SplitData/test_dir\"\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_dir,target_size=(208,176),batch_size=20,class_mode='categorical')\n",
    "test_generator=test_datagen.flow_from_directory(test_dir,target_size=(208,176),batch_size=20,class_mode='categorical')\n",
    "valid_generator=valid_datagen.flow_from_directory(valid_dir,target_size=(208,176),batch_size=20,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c6e0a",
   "metadata": {},
   "source": [
    "On a bien le bon nombre de label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0f4b",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "### Modele de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6b80a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_42 (Conv2D)          (None, 206, 174, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 103, 87, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 101, 85, 64)       18496     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 101, 85, 64)       0         \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 50, 42, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 48, 40, 128)       73856     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 48, 40, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPoolin  (None, 24, 20, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 22, 18, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_44 (MaxPoolin  (None, 11, 9, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 12672)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               6488576   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,731,460\n",
      "Trainable params: 6,731,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=(208,176,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "model.add(layers.Dense(4,activation='softmax'))\n",
    "\n",
    "model.build(input_shape=(208, 176, 3))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=[\"acc\"])\n",
    "\n",
    "#tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bd94b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ranus\\AppData\\Local\\Temp\\ipykernel_32716\\2605433350.py\", line 1, in <cell line: 1>\n      history=model.fit(train_generator,validation_data=valid_generator)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [20,4] and labels shape [80]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_9922]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ranus\\AppData\\Local\\Temp\\ipykernel_32716\\2605433350.py\", line 1, in <cell line: 1>\n      history=model.fit(train_generator,validation_data=valid_generator)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\ranus\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [20,4] and labels shape [80]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_9922]"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_generator,validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30371aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path2=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset\"\n",
    "#image=[]\n",
    "#categorie=[]\n",
    "#for dirname, _,filenames in os.walk(path2):\n",
    " #   for filename in filenames:\n",
    "  #      image+=[os.path.join(dirname, filename)]\n",
    "   #     categorie+=[dirname.split(\"\\\\\")[-1]]\n",
    "\n",
    "#df=pd.DataFrame({\"images\":image,\"categories\":categorie})\n",
    "#df=df.sample(frac=1)\n",
    "#df\n",
    "#df\n",
    "#np.random.shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f0ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"categories\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465104cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15431c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7673d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff39352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155406b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb7c11a",
   "metadata": {},
   "source": [
    "### Traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#train_datagen=ImageDataGenerator(rescale=1./255)\n",
    "#test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "data_test\n",
    "def traitement(data):\n",
    "    for i in range(0,data.shape[0]-1):\n",
    "        images+=[cv2.imread(data.iloc[i:0])/255]\n",
    "        labels+=[data.iloc[i:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c802d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff481f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "path=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset\"\n",
    "folder_path=[]\n",
    "label=[]\n",
    "for dirname,_, filename in os.walk(path):\n",
    "    folder_path.append(dirname)\n",
    "    label.append(dirname.split('/')[-1])\n",
    "    \n",
    "del folder_path[0] #On retire le 1er élément qui correspond au chemin path\n",
    "\n",
    "for i in range(len(folder_path)):\n",
    "    folder_path[i]=re.sub(\"\\\\\\\\\",\"/\",folder_path[i]) #On corrige les antislash en slash\n",
    "\n",
    "    \n",
    "#On renomme les fichiers\n",
    "for pat in folder_path:\n",
    "    j=0 #initalisationde l'accumulateur\n",
    "    for dirname ,_, filenames  in os.walk(pat):\n",
    "        a=re.sub(\"\\\\\\\\\",\"/\",os.path.join(pat,filenames[j]))\n",
    "        b=re.sub(\"\\\\\\\\\",\"/\",os.path.join(pat.split(\"/\")[-1],str(j)+'.jpg'))\n",
    "        j+=1\n",
    "        #print(a)\n",
    "        #print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Users/ranus/alzheimer/archive(1)/OriginalDataset\"\n",
    "folder_path=[]\n",
    "for dirname,_, filename in os.walk(path):\n",
    "    folder_path.append(dirname)\n",
    "    \n",
    "del folder_path[0] #On retire le 1er élément qui correspond au chemin path\n",
    "\n",
    "for i in range(len(folder_path)):\n",
    "    folder_path[i]=re.sub(\"\\\\\\\\\",\"/\",folder_path[i]) #On corrige les antislash en slash\n",
    "\n",
    "    \n",
    "#On renomme les fichiers\n",
    "for pat in folder_path:\n",
    "    j=0 #initalisationde l'accumulateur\n",
    "    for dirname ,_, filenames  in os.walk(pat):\n",
    "        for file in filenames:\n",
    "            a=os.path.join(pat,file)\n",
    "            print(re.sub(\"\\\\\\\\\",\"/\",a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbbc36",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeadda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(p0)[1]\n",
    "\n",
    "re.sub(\"\\\\\\\\\",\"/\",os.path.join(p0,fnames[k]))\n",
    "re.sub(\"\\\\\\\\\",\"/\",os.path.join(p,str(k)+\".jpg\"))\n",
    "cat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
